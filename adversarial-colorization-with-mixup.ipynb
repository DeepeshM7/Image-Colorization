{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\n\nimport numpy as np\nimport cv2\nfrom PIL import Image\nfrom pathlib import Path\n# from tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom skimage.color import rgb2lab, lab2rgb\nimport cv2\nimport torchvision\nfrom torchvision import models\nimport torch.nn.functional as F\n\n# import scipy\nimport torch\nimport torchvision.datasets as dset\nfrom torch import nn, optim\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nnp.random.seed(2021)\ntorch.manual_seed(2021)\nimport copy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-06T10:40:46.261963Z","iopub.execute_input":"2022-08-06T10:40:46.262568Z","iopub.status.idle":"2022-08-06T10:40:48.944730Z","shell.execute_reply.started":"2022-08-06T10:40:46.262488Z","shell.execute_reply":"2022-08-06T10:40:48.943866Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# **Defining Dataset**","metadata":{}},{"cell_type":"code","source":"data_root = \"../input/flickrfaceshq-dataset-ffhq\"\n\nclass FaceData(torch.utils.data.Dataset):\n    def __init__(self, transformations=None, root = '../input/flickrfaceshq-dataset-ffhq'):\n        self.root = root\n        self.transformations = transformations\n        self.img_list = glob.glob(os.path.join(root,'*'))\n        self.img_list.sort()\n    \n    def __len__(self):\n        return len(glob.glob(\"../input/flickrfaceshq-dataset-ffhq/*\"))\n\n    def mask(self, img):\n        canvas = np.full((224,224,3), 255, np.uint8) #draw white canvas\n        \n        for _ in range(np.random.randint(1,10)):\n            x1, x2 = np.random.randint(1,224), np.random.randint(1,224)\n            y1, y2 = np.random.randint(1,224), np.random.randint(1,224)\n            t = np.random.randint(1,3)\n            \n            cv2.line(canvas,(x1,y1),(x2,y2),(0,0,0),t)\n            \n            \n        masked_img = img.copy()\n        masked_img[canvas==0] = 255\n            \n        return masked_img, canvas\n    \n    def __getitem__(self, idx):\n        ip_img = Image.open(self.img_list[idx])#.convert(\"RGB\")\n        ip_img = ip_img.resize((224,224))\n        \n        ip_img = np.array(ip_img)\n        img_lab = rgb2lab(ip_img).astype(\"float32\") # Converting RGB to L*a*b\n        img_lab = transforms.ToTensor()(img_lab)\n\n        L = img_lab[[0]] / 50. - 1. # Max for L is 100 so need /50 -1 to get b/w -1,1\n        ab = img_lab[[1,2]]/100. #Max for a and b is 127\n        \n        masked_L = copy.deepcopy(ip_img)\n        masked_L,_ = self.mask(masked_L)\n\n        masked_L = rgb2lab(masked_L).astype(\"float32\")\n        masked_L = transforms.ToTensor()(masked_L)\n        masked_L = masked_L[[0]] / 50. - 1.\n        \n        return L, ab, masked_L\n        ","metadata":{"execution":{"iopub.status.busy":"2022-08-06T10:40:49.733243Z","iopub.execute_input":"2022-08-06T10:40:49.733575Z","iopub.status.idle":"2022-08-06T10:40:49.750326Z","shell.execute_reply.started":"2022-08-06T10:40:49.733539Z","shell.execute_reply":"2022-08-06T10:40:49.748933Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data_set = FaceData()\n\n# sample_loader = torch.utils.data.DataLoader(data_set, batch_size=32)\n\n# sample_batch=next(iter(sample_loader))\n# len(sample_batch)\n\n# type(sample_batch[0]),sample_batch[0].shape,sample_batch[1].shape,sample_batch[2].shape\n# concat_lab=torch.cat((sample_batch[0],sample_batch[1]),dim=1)\n# concat_lab.shape\n# concat_rgb = lab2rgb(torch.cat(((sample_batch[0]+1)*50.,sample_batch[1]*100.),dim=1).permute(0,2,3,1))\n\n# concat_rgb = torch.tensor(concat_rgb).permute(0,3,1,2)\n\n# grid1 = torchvision.utils.make_grid(sample_batch[0],nrow=16)\n# grid2 = torchvision.utils.make_grid(concat_lab,nrow=16)\n# grid3 =  torchvision.utils.make_grid(concat_rgb,nrow=16)\n# grid4 =  torchvision.utils.make_grid(sample_batch[2],nrow=16)\n\n# fig, axs = plt.subplots(4,1,figsize=(50,50),sharex=True)#2 rows, 1 column\n\n# axs[0].imshow(np.transpose(grid1,(1,2,0)))\n# axs[1].imshow(np.transpose(grid2,(1,2,0)))\n# axs[2].imshow(np.transpose(grid3,(1,2,0)))\n# axs[3].imshow(np.transpose(grid4,(1,2,0)))\n\n# plt.subplots_adjust(top=0.5)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T10:40:53.177323Z","iopub.execute_input":"2022-08-06T10:40:53.178065Z","iopub.status.idle":"2022-08-06T10:40:55.924860Z","shell.execute_reply.started":"2022-08-06T10:40:53.178030Z","shell.execute_reply":"2022-08-06T10:40:55.924054Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# indices = list(range(len(data_set.img_list)))\n# val_split = int(0.9*len(data_set.img_list)) \n# test_split = int(val_split+ ((len(data_set.img_list)-val_split)//2))\nindices = list(range(30000))\n\n# train_indices,val_indices,test_indices = indices[:20000], indices[20000:25000],indices[25000:30000]\ntrain_indices,val_indices,test_indices = indices[:20000], indices[20000:25000],indices[25000:30000]\n\ntrain_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices)\nval_sampler = torch.utils.data.sampler.SequentialSampler(val_indices)\ntest_sampler = torch.utils.data.sampler.SubsetRandomSampler(test_indices)\n\ntrain_data = torch.utils.data.DataLoader(data_set,batch_size =32, sampler=train_sampler,\n                                        num_workers = 2)\nval_data = torch.utils.data.DataLoader(data_set,batch_size = 32, sampler =val_sampler,\n                                      num_workers =2)\ntest_data = torch.utils.data.DataLoader(data_set,batch_size = 32, sampler =test_sampler,\n                                      num_workers =2)\nprint(f'Train_data_size={len(train_data)}; Val_data_size={len(val_data)} ; Test_data_size={len(test_data)}')","metadata":{"execution":{"iopub.status.busy":"2022-08-06T10:40:55.926434Z","iopub.execute_input":"2022-08-06T10:40:55.926715Z","iopub.status.idle":"2022-08-06T10:40:55.937696Z","shell.execute_reply.started":"2022-08-06T10:40:55.926681Z","shell.execute_reply":"2022-08-06T10:40:55.935954Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **Defining Model**","metadata":{}},{"cell_type":"code","source":"class Image_Colorization_Model(nn.Module):\n    def __init__(self):\n        super(Image_Colorization_Model, self).__init__()\n#         Encoder\n        self.conv_preprocess1 = nn.Conv2d(1, 3, kernel_size=3, padding=1)\n        self.conv_preprocess2 = nn.Conv2d(3, 3, kernel_size=3, padding=1)\n        model_resnet = models.resnet50(pretrained=True)\n        self.conv1 = model_resnet.conv1\n        self.bn1 = model_resnet.bn1\n        self.relu = model_resnet.relu\n        self.maxpool = model_resnet.maxpool\n        self.layer1 = model_resnet.layer1\n        self.layer2 = model_resnet.layer2\n#         Decoder\n        self.upsample = nn.Upsample(scale_factor = 2, mode = 'nearest')\n        self.conv_decode2_1 = nn.Conv2d(768, 128, kernel_size=3, padding=1)\n        self.conv_decode2_2 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n        \n        self.conv_decode1_1 = nn.Conv2d(128, 16, kernel_size=3, padding=1)\n        self.conv_decode1_2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n        \n        self.conv_decode0_1 = nn.Conv2d(11, 4, kernel_size=3, padding=1)\n        self.conv_decode0_2 = nn.Conv2d(4, 2, kernel_size=1)\n        \n        #       Inpainting_Branch\n        self.seg_upsample = nn.Upsample(scale_factor = 2, mode = 'nearest')\n        self.seg_conv_decode2_1 = nn.Conv2d(768, 128, kernel_size=3, padding=1)\n        self.seg_conv_decode2_2 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n        \n        self.seg_conv_decode1_1 = nn.Conv2d(128, 16, kernel_size=3, padding=1)\n        self.seg_conv_decode1_2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n        \n        self.seg_conv_decode0_1 = nn.Conv2d(11, 4, kernel_size=3, padding=1)\n        self.seg_conv_decode0_2 = nn.Conv2d(4, 1, kernel_size=1)\n        \n        \n        \n    def forward(self,x, eval_flag=False):\n        ######ENCODER############\n        x_pre = F.relu(self.conv_preprocess1(x))\n        x_pre = F.relu(self.conv_preprocess2(x_pre))\n        \n        encode_1 = self.conv1(x_pre)\n        encode_1 = self.bn1(encode_1)\n        encode_1 = self.relu(encode_1)\n        \n        x_mp = self.maxpool(encode_1)\n        \n        encode_2 = self.layer1(x_mp)\n        bottle_neck = self.layer2(encode_2)\n        \n        \n        #########Inpainting_Branch#########\n        if not eval_flag:\n            seg_decode_2 = self.upsample(bottle_neck[bottle_neck.size(0)//2:])\n            seg_decode_2 = torch.cat((encode_2[encode_2.size(0)//2:],seg_decode_2),dim=1)\n    #         print(seg_decode_2.shape)\n            seg_decode_2 = F.relu(self.seg_conv_decode2_1(seg_decode_2))\n            seg_decode_2 = F.relu(self.seg_conv_decode2_2(seg_decode_2))\n\n            seg_decode_1 = self.upsample(seg_decode_2)\n            seg_decode_1 = torch.cat((encode_1[encode_1.size(0)//2:],seg_decode_1),dim=1)\n            seg_decode_1 = F.relu(self.seg_conv_decode1_1(seg_decode_1))\n            seg_decode_1 = F.relu(self.seg_conv_decode1_2(seg_decode_1))\n\n            seg_decode_pre = self.upsample(seg_decode_1)\n            seg_decode_pre = torch.cat((x_pre[x_pre.size(0)//2:],seg_decode_pre),dim=1)\n            seg_decode_pre = F.relu(self.seg_conv_decode0_1(seg_decode_pre))\n            seg_decode_pre = (self.seg_conv_decode0_2(seg_decode_pre))\n            \n        ########DECODER#########\n        if eval_flag:\n            decode_2 = self.upsample(bottle_neck)\n\n            decode_2 = torch.cat((encode_2,decode_2),dim=1)\n        else:          \n            decode_2 = self.upsample(bottle_neck[:bottle_neck.size(0)//2])\n\n            decode_2 = torch.cat((encode_2[:encode_2.size(0)//2],decode_2),dim=1)\n            \n#         print(decode_2.shape)\n        decode_2 = F.relu(self.conv_decode2_1(decode_2))\n        decode_2 = F.relu(self.conv_decode2_2(decode_2))\n        \n        if not eval_flag:\n            ''''''\n            decode_2 = seg_decode_2 * decode_2\n            ''''''\n        decode_1 = self.upsample(decode_2)\n        \n        if eval_flag:\n            decode_1 = torch.cat((encode_1,decode_1),dim=1)\n        else:\n            decode_1 = torch.cat((encode_1[:encode_1.size(0)//2],decode_1),dim=1)\n        \n        decode_1 = F.relu(self.conv_decode1_1(decode_1))\n        decode_1 = F.relu(self.conv_decode1_2(decode_1))\n        \n        if not eval_flag:\n            ''''''\n            decode_1 = seg_decode_1 * decode_1\n            ''''''\n        decode_pre = self.upsample(decode_1)\n        \n        if eval_flag:\n            decode_pre = torch.cat((x_pre,decode_pre),dim=1)\n        else:\n            decode_pre = torch.cat((x_pre[:x_pre.size(0)//2],decode_pre),dim=1)\n        \n        decode_pre = F.relu(self.conv_decode0_1(decode_pre))\n        decode_pre = F.relu(self.conv_decode0_2(decode_pre))\n        \n        if eval_flag:\n            return decode_pre\n        else: \n            return decode_pre, seg_decode_pre\n\nclass Descriminator(nn.Module):\n    def __init__(self):\n        super(Descriminator, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=4,stride=2, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        \n        self.conv2 = nn.Conv2d(64, 128, kernel_size=4,stride=2, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        \n        self.conv3 = nn.Conv2d(128, 256, kernel_size=4,stride=2, padding=1)\n        self.bn3 = nn.BatchNorm2d(256)\n        \n        self.conv4 = nn.Conv2d(256, 512, kernel_size=4,stride=1, padding=1)\n        self.bn4 = nn.BatchNorm2d(512)\n        \n        self.conv5 = nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n    \n    def forward(self, x):\n        x = F.leaky_relu(self.bn1(self.conv1(x)),negative_slope=0.2)\n        x = F.leaky_relu(self.bn2(self.conv2(x)),negative_slope=0.2)\n        x = F.leaky_relu(self.bn3(self.conv3(x)),negative_slope=0.2)\n        x = F.leaky_relu(self.bn4(self.conv4(x)),negative_slope=0.2)\n        \n        x = self.conv5(x)\n        \n        return x\n        ","metadata":{"execution":{"iopub.status.busy":"2022-08-06T10:40:55.939161Z","iopub.execute_input":"2022-08-06T10:40:55.939604Z","iopub.status.idle":"2022-08-06T10:40:55.972115Z","shell.execute_reply.started":"2022-08-06T10:40:55.939560Z","shell.execute_reply":"2022-08-06T10:40:55.971369Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# discriminator = Descriminator()\n# dummy_input = torch.randn(4, 3, 256, 256) # batch_size, channels, size, size\n# out = discriminator(dummy_input)\n# out.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-06T10:40:55.973944Z","iopub.execute_input":"2022-08-06T10:40:55.974361Z","iopub.status.idle":"2022-08-06T10:40:55.984243Z","shell.execute_reply.started":"2022-08-06T10:40:55.974325Z","shell.execute_reply":"2022-08-06T10:40:55.983466Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nlearning_rate = 1e-3\n# encoder_model = ResNet().to(device)\n# decoder_model = Decoder(device).to(device)\n# print(next(decoder_model.parameters()).device)\nmodel = Image_Colorization_Model().cuda()\nmodel_D = Descriminator().cuda()\n# model = model.to(device)\noptimizer_G = torch.optim.Adam(model.parameters(), lr=learning_rate)\noptimizer_D = torch.optim.Adam(model_D.parameters(), lr=learning_rate)\nL1loss = nn.L1Loss()\nDisc_loss = nn.BCEWithLogitsLoss()\n# os.mkdir('./weights')","metadata":{"execution":{"iopub.status.busy":"2022-08-06T10:40:57.313681Z","iopub.execute_input":"2022-08-06T10:40:57.314372Z","iopub.status.idle":"2022-08-06T10:41:11.532121Z","shell.execute_reply.started":"2022-08-06T10:40:57.314339Z","shell.execute_reply":"2022-08-06T10:41:11.531269Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# **Training Colorization Model**","metadata":{}},{"cell_type":"code","source":"os.mkdir('./weights')\ndef plot_loss_L1(train_list):\n    plt.figure(figsize=(20,20))\n    plt.plot(train_list,label='Train loss L1')\n    plt.xlabel('Iteration')\n    plt.ylabel('L1 Loss')\n    plt.legend(loc='upper right')\n    plt.savefig(os.path.join('./', 'Loss_graph_L1'))\n    plt.close()\n\ndef plot_loss_mse(train_list):\n    plt.figure(figsize=(20,20))\n    plt.plot(train_list,label='Train loss MSE')\n    plt.xlabel('Iteration')\n    plt.ylabel('MSE Loss')\n    plt.legend(loc='upper right')\n    plt.savefig(os.path.join(os.path.join('./', 'Loss_graph_mse')))\n    plt.close()\n\ndef train_discriminator(model_D, gray, predicted_ab_space, true_ab_space):\n    fake_img = torch.cat((gray,predicted_ab_space),dim=1)\n    disk_pred_fake = model_D(fake_img.detach())\n    fake_label = torch.zeros_like(disk_pred_fake)\n    Disc_loss_fake = Disc_loss(disk_pred_fake, fake_label)\n#     print(disk_pred_fake.shape,fake_label.shape)\n    \n    real_img = torch.cat((gray,true_ab_space),dim=1)\n    disk_pred_real = model_D(real_img)\n    real_label = torch.ones_like(disk_pred_real)\n    Disc_loss_real = Disc_loss(disk_pred_real, real_label)\n#     print(disk_pred_real.shape,real_label.shape)\n    \n#     print(Disc_loss_fake,Disc_loss_real)\n    return (Disc_loss_fake + Disc_loss_real)*0.5\n\ndef train_generator(model_D, gray, predicted_ab_space):\n    fake_img = torch.cat((gray,predicted_ab_space),dim=1)\n    disk_pred_fake = model_D(fake_img)\n    real_label = torch.ones_like(disk_pred_fake)\n    adversarial_loss = Disc_loss(disk_pred_fake, real_label)\n#     print(disk_pred_fake.shape,real_label.shape)\n    \n    return adversarial_loss\n\n# '''    \nchkpt = torch.load(\"../input/55-epoch-ad-color-mix/_epoch_55 (2)\")\n\nmodel.load_state_dict(chkpt['model_state_dict'])\nmodel_D.load_state_dict(chkpt['model_D_state_dict'])\noptimizer_D.load_state_dict(chkpt['optimizer_D_state_dict'])\noptimizer_G.load_state_dict(chkpt['optimizer_G_state_dict'])\nchkpt_epoch = chkpt['epoch']\n\ntrain_loss_l1 = chkpt['loss']['L1_Loss'].copy()\ntrain_loss_mse = chkpt['loss']['MSE_Loss'].copy()\ntrain_loss_inpaint = chkpt['loss']['inpaint_Loss'].copy()\ntotal_loss = chkpt['loss']['total_Loss'].copy()\ntrain_loss_ad = chkpt['loss']['train_loss_ad'].copy()\ntrain_disc_loss = chkpt['loss']['train_disc_loss'].copy()\n\n# '''\n\nmodel.train()\nmodel_D.train()\n\n# train_loss_l1 = []\n# train_loss_mse = []\n# train_loss_inpaint = []\n# total_loss = []\n# train_loss_ad = []\n# train_disc_loss = []\n\nfor i in range(70):#Total=70 epoch\n#     b=0\n    for data in train_data:\n#         print(len(data))\n        model.train()\n        model_D.train()\n        gray = data[0].to(device)  \n#         plt.imshow(gray)\n        true_ab_space = data[1].to(device)\n        masked_L = data[2].to(device)\n        \n        input_cat = torch.cat((gray, masked_L))\n        \n        \n        optimizer_D.zero_grad()\n#         print(f'x:{gray.shape}')\n#         print(f'y:{true_ab_space.shape}')\n#         print(f'z:{masked_L.shape}')\n#         print(f'zzz:{input_cat.shape}')\n    \n        predicted_ab_space, inpainted_img = model(input_cat)\n        \n#         fake_img = torch.cat((gray,predicted_ab_space),dim=1)\n#         disk_pred_fake = model_D(fake_img.detach())\n#         fake_label = torch.zeros_like(disk_pred_fake)\n#         Disc_loss_fake = Disc_loss(disk_pred_fake, fake_label)\n        \n#         real_img = torch.cat((gray,true_ab_space),dim=1)\n#         disk_pred_real = model_D(real_img.detach())\n#         real_label = torch.ones_like(disk_pred_real)\n#         Disc_loss_real = Disc_loss(disk_pred_real, real_label)\n        \n        for param in model_D.parameters():\n            param.requires_grad = True\n            \n        discriminator_loss = train_discriminator(model_D, gray, predicted_ab_space, true_ab_space)\n        discriminator_loss.backward()\n        \n        optimizer_D.step()\n        \n#         print(predicted_ab_space.shape)\n#         print(true_ab_space.shape)\n        optimizer_G.zero_grad()\n        \n        for param in model_D.parameters():\n            param.requires_grad = False\n        \n        model_D.eval()\n        ad_loss = train_generator(model_D, gray, predicted_ab_space)\n        loss_color = L1loss(predicted_ab_space, true_ab_space) * 100.\n        loss_inpaint = L1loss(inpainted_img, gray) * 100.\n        \n        gen_loss = ad_loss + loss_color + loss_inpaint\n        \n        gen_loss.backward()\n        \n        optimizer_G.step()\n    \n        \n#         loss_inpaint = L1loss(inpainted_img, gray)\n        \n#         loss = loss_color + loss_inpaint\n    \n        train_loss_l1.append(loss_color.item())\n        train_loss_mse.append((F.mse_loss(predicted_ab_space, true_ab_space)).item())\n        train_loss_inpaint.append(loss_inpaint.item())\n        \n        train_loss_ad.append(ad_loss.item())\n        train_disc_loss.append(discriminator_loss.item())\n        \n        total_loss.append(gen_loss.item())\n\n#         print(loss)\n#         b+=1\n#         print(b)\n        \n#         plot_loss_L1(train_loss_l1)\n#         plot_loss_mse(train_loss_mse)\n        \n    if (i+1) % 1 == 0:\n        print('%d iterations' % (i+1))\n        print('L1_Loss %.3f' % np.mean(train_loss_l1[-100:]))\n        print('MSE_Loss: %.3f' % np.mean(train_loss_mse[-100:]))\n        print('train_loss_inpaint: %.3f' % np.mean(train_loss_inpaint[-100:]))\n        print('train_loss_ad: %.3f' % np.mean(train_loss_ad[-100:]))\n        print('train_disc_loss: %.3f' % np.mean(train_disc_loss[-100:]))\n        print('total_loss: %.3f' % np.mean(total_loss[-100:]))\n\n\n    if (i+1)%5 == 0:\n        torch.save({'epoch':i,\n                    'model_state_dict':model.state_dict(),\n                    'model_D_state_dict':model_D.state_dict(),\n                    'optimizer_D_state_dict':optimizer_D.state_dict(),\n                    'optimizer_G_state_dict':optimizer_G.state_dict(),\n                    'loss':{'L1_Loss':train_loss_l1.copy(),'MSE_Loss':train_loss_mse.copy(),'inpaint_Loss':train_loss_inpaint.copy(),'train_loss_ad':train_loss_ad.copy(),'train_disc_loss':train_disc_loss.copy(),'total_Loss':total_loss.copy()},\n                   },os.path.join('./weights',f'_epoch_{i+56}'))","metadata":{"execution":{"iopub.status.busy":"2022-05-28T20:03:16.0965Z","iopub.status.idle":"2022-05-28T20:03:16.09702Z","shell.execute_reply.started":"2022-05-28T20:03:16.096736Z","shell.execute_reply":"2022-05-28T20:03:16.096765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# val_loss_L1 = []\n# val_loss_mse = []\n# model.eval()\n# with torch.no_grad():\n#     for i,data in enumerate(val_data):\n#             print(i)\n            \n#             gray = data[0].to(device)  \n\n#             true_ab_space = data[1].to(device)\n#             masked_L = data[2].to(device)\n\n#             input_cat = torch.cat((gray, masked_L))\n            \n#             predicted_ab_space,_ = model(input_cat)\n# #             predicted_ab_space = model(gray,eval_flag=True)\n            \n\n#             val_loss_L1.append(L1loss(predicted_ab_space, true_ab_space).item())\n#             val_loss_mse.append((F.mse_loss(predicted_ab_space, true_ab_space)).item())\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-23T12:17:19.801Z","iopub.execute_input":"2022-05-23T12:17:19.801264Z","iopub.status.idle":"2022-05-23T12:17:19.805822Z","shell.execute_reply.started":"2022-05-23T12:17:19.801236Z","shell.execute_reply":"2022-05-23T12:17:19.804726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mean_val_loss_l1 = sum(val_loss_L1)/len(val_loss_L1)\n# mean_val_loss_mse = sum(val_loss_mse)/len(val_loss_mse)\n\n# mean_val_loss_l1,mean_val_loss_mse","metadata":{"execution":{"iopub.status.busy":"2022-05-23T12:11:25.025199Z","iopub.execute_input":"2022-05-23T12:11:25.025512Z","iopub.status.idle":"2022-05-23T12:11:25.033896Z","shell.execute_reply.started":"2022-05-23T12:11:25.025476Z","shell.execute_reply":"2022-05-23T12:11:25.03311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Testing Trained Model**","metadata":{}},{"cell_type":"code","source":"# torch.cuda.memory_allocated(), torch.cuda.current_device()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T02:22:48.070445Z","iopub.execute_input":"2022-01-12T02:22:48.07216Z","iopub.status.idle":"2022-01-12T02:22:48.089144Z","shell.execute_reply.started":"2022-01-12T02:22:48.072091Z","shell.execute_reply":"2022-01-12T02:22:48.087683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chkpt = torch.load(\"../input/adversarial-mixup-105-epoch/_epoch_105\",map_location=torch.device('cpu'))\n# chkpt_inpaint = torch.load(\"../input/inpaint35/inpaint_epoch_35 (1)\",map_location=torch.device('cpu'))\nmodel.load_state_dict(chkpt['model_state_dict'])\nmodel_D.load_state_dict(chkpt['model_D_state_dict'])\n\n# inpaint_model.load_state_dict(chkpt_inpaint['model_state_dict'])\n\nmodel.eval()\n\nwith torch.no_grad():\n    data_iterator = iter(val_data)\n    for _ in range(7):\n        (gray_val, true_ab_val,masked_L) = next(data_iterator)\n    #     show(gray_val.cpu().numpy().transpose(0,2,3,1)[0])\n\n        gray_val_orig = copy.deepcopy(gray_val)\n        true_ab_val_orig = copy.deepcopy(true_ab_val)\n\n        gray_val = gray_val.to(device)\n        masked_L = masked_L.to(device)\n        true_ab_val = true_ab_val.to(device)\n\n        pred_ab, pred_inpaint = model(torch.cat((gray_val,masked_L)))\n\n        fake_img = torch.cat((gray_val,pred_ab),dim=1)\n        real_img = torch.cat((gray_val,true_ab_val),dim=1)\n        disk_pred_fake = model_D(fake_img)\n        disk_pred_real = model_D(real_img)\n\npred_ab = (pred_ab*100.).permute(0, 2, 3, 1).contiguous()\ngray_val = ((gray_val+1)*50.).permute(0,2,3,1).contiguous()\n\nab_rgb = (true_ab_val_orig*100.).permute(0, 2, 3, 1).contiguous()\ngray_rgb = ((gray_val_orig+1)*50.).permute(0,2,3,1).contiguous()\n\ntrue_rgb = torch.tensor(lab2rgb(torch.cat((gray_rgb,ab_rgb),dim=3).cpu()))\ntrue_rgb = true_rgb.permute(0,3,1,2)\n# gray_val_orig = gray_val_orig.cpu().numpy().transpose(0,2,3,1)\n\n# pred_ab = (pred_ab*100.).cpu().numpy().transpose(0,2,3,1)\n# gray_val = ((gray_val+1)*50.).cpu().numpy().transpose(0,2,3,1)\n# pred_lab = np.concatenate((gray_val,pred_ab),axis=3)\n# print(pred_ab.shape,gray_val.shape,pred_lab.shape)\n# pred_rgb =  lab2rgb(pred_lab)\n\npred_rgb = torch.tensor(lab2rgb(torch.cat((gray_val,pred_ab),dim=3).cpu()))\npred_rgb = pred_rgb.permute(0,3,1,2)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T10:41:11.534015Z","iopub.execute_input":"2022-08-06T10:41:11.534437Z","iopub.status.idle":"2022-08-06T10:41:24.396306Z","shell.execute_reply.started":"2022-08-06T10:41:11.534395Z","shell.execute_reply":"2022-08-06T10:41:24.394744Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from torchvision.utils import make_grid\ndef show(img,figure_size=(20,20)):\n#     print(img.shape)\n    npimg = img.numpy()\n    npimg = img\n    print(npimg.shape)\n    _, ax = plt.subplots(figsize=figure_size)\n    fig = ax.imshow(np.transpose(npimg, (1,2,0)))\n    fig.axes.get_xaxis().set_visible(True)\n    fig.axes.get_yaxis().set_visible(True)\n    plt.savefig(\"./output\")\n\nshow(make_grid(((gray_val/100.)).permute(0,3,1,2).cpu().data))","metadata":{"execution":{"iopub.status.busy":"2022-08-06T10:41:24.400634Z","iopub.execute_input":"2022-08-06T10:41:24.400896Z","iopub.status.idle":"2022-08-06T10:41:26.949801Z","shell.execute_reply.started":"2022-08-06T10:41:24.400868Z","shell.execute_reply":"2022-08-06T10:41:26.948583Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"show(make_grid(pred_rgb.cpu().data) )\n# show(pred_rgb[0])","metadata":{"execution":{"iopub.status.busy":"2022-08-06T10:41:31.087018Z","iopub.execute_input":"2022-08-06T10:41:31.087280Z","iopub.status.idle":"2022-08-06T10:41:32.928173Z","shell.execute_reply.started":"2022-08-06T10:41:31.087249Z","shell.execute_reply":"2022-08-06T10:41:32.926497Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# disk_pred_fake=disk_pred_fake>0.5\n# disk_pred_fake","metadata":{"execution":{"iopub.status.busy":"2022-06-08T04:45:28.615893Z","iopub.execute_input":"2022-06-08T04:45:28.616147Z","iopub.status.idle":"2022-06-08T04:45:28.619863Z","shell.execute_reply.started":"2022-06-08T04:45:28.616119Z","shell.execute_reply":"2022-06-08T04:45:28.618947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# disk_pred_real=disk_pred_real>0.5\n# disk_pred_real","metadata":{"execution":{"iopub.status.busy":"2022-06-08T04:45:29.782639Z","iopub.execute_input":"2022-06-08T04:45:29.782887Z","iopub.status.idle":"2022-06-08T04:45:29.786437Z","shell.execute_reply.started":"2022-06-08T04:45:29.782858Z","shell.execute_reply":"2022-06-08T04:45:29.785387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show(make_grid(pred_rgb.cpu().data) )","metadata":{"execution":{"iopub.status.busy":"2022-06-08T04:45:30.030713Z","iopub.execute_input":"2022-06-08T04:45:30.030968Z","iopub.status.idle":"2022-06-08T04:45:30.034951Z","shell.execute_reply.started":"2022-06-08T04:45:30.03094Z","shell.execute_reply":"2022-06-08T04:45:30.034213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show(make_grid(true_rgb.cpu().data) )","metadata":{"execution":{"iopub.status.busy":"2022-06-08T05:43:41.391908Z","iopub.execute_input":"2022-06-08T05:43:41.392607Z","iopub.status.idle":"2022-06-08T05:43:43.200089Z","shell.execute_reply.started":"2022-06-08T05:43:41.39257Z","shell.execute_reply":"2022-06-08T05:43:43.199283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:46:45.614424Z","iopub.execute_input":"2022-05-31T01:46:45.615998Z","iopub.status.idle":"2022-05-31T01:46:47.001833Z","shell.execute_reply.started":"2022-05-31T01:46:45.615941Z","shell.execute_reply":"2022-05-31T01:46:47.001208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show(make_grid(pred_inpaint.cpu().data) )","metadata":{"execution":{"iopub.status.busy":"2022-05-22T21:18:30.504874Z","iopub.execute_input":"2022-05-22T21:18:30.505138Z","iopub.status.idle":"2022-05-22T21:18:32.112316Z","shell.execute_reply.started":"2022-05-22T21:18:30.505111Z","shell.execute_reply":"2022-05-22T21:18:32.111654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show(make_grid(masked_L.cpu().data) )","metadata":{"execution":{"iopub.status.busy":"2022-05-22T20:41:19.318625Z","iopub.execute_input":"2022-05-22T20:41:19.318898Z","iopub.status.idle":"2022-05-22T20:41:20.493206Z","shell.execute_reply.started":"2022-05-22T20:41:19.318868Z","shell.execute_reply":"2022-05-22T20:41:20.492581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install torch-summary","metadata":{"execution":{"iopub.status.busy":"2022-01-12T02:23:03.996509Z","iopub.execute_input":"2022-01-12T02:23:03.997421Z","iopub.status.idle":"2022-01-12T02:23:16.694291Z","shell.execute_reply.started":"2022-01-12T02:23:03.997377Z","shell.execute_reply":"2022-01-12T02:23:16.692991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from torchsummary import summary\n# summary(model, (1, 224, 224))","metadata":{"execution":{"iopub.status.busy":"2022-01-12T02:23:16.696678Z","iopub.execute_input":"2022-01-12T02:23:16.697059Z","iopub.status.idle":"2022-01-12T02:23:16.902598Z","shell.execute_reply.started":"2022-01-12T02:23:16.696998Z","shell.execute_reply":"2022-01-12T02:23:16.901403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss = chkpt['loss']['L1_Loss']\n# print(len(loss))\n# f = plt.figure(figsize=(20,8))\n# ax = f.add_subplot(1,2,1)\n# ax.plot(loss)\n# # ax.set_yscale('log')\n# ax.set_title('L1 Loss')\n# ax.set_xlabel('Iteration')\n# f.savefig('./loss')","metadata":{"execution":{"iopub.status.busy":"2022-05-15T23:38:20.997361Z","iopub.execute_input":"2022-05-15T23:38:20.997906Z","iopub.status.idle":"2022-05-15T23:38:21.325128Z","shell.execute_reply.started":"2022-05-15T23:38:20.99787Z","shell.execute_reply":"2022-05-15T23:38:21.324473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print('L1_Loss %.3f' % np.mean(loss[-100:]))","metadata":{"execution":{"iopub.status.busy":"2022-05-15T23:39:17.249077Z","iopub.execute_input":"2022-05-15T23:39:17.249349Z","iopub.status.idle":"2022-05-15T23:39:17.253806Z","shell.execute_reply.started":"2022-05-15T23:39:17.249319Z","shell.execute_reply":"2022-05-15T23:39:17.2529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss = chkpt['loss']['MSE_Loss']\n# f = plt.figure(figsize=(20,8))\n# ax = f.add_subplot(1,2,1)\n# ax.plot(loss)\n# # ax.set_yscale('log')\n# ax.set_title('MSE_Loss Loss')\n# ax.set_xlabel('Iteration')","metadata":{"execution":{"iopub.status.busy":"2022-01-12T02:23:17.296768Z","iopub.execute_input":"2022-01-12T02:23:17.29837Z","iopub.status.idle":"2022-01-12T02:23:17.602005Z","shell.execute_reply.started":"2022-01-12T02:23:17.298321Z","shell.execute_reply":"2022-01-12T02:23:17.601029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss = chkpt['loss']['train_disc_loss'][:200]\n# f = plt.figure(figsize=(20,8))\n# ax = f.add_subplot(1,2,1)\n# ax.plot(loss)\n# # ax.set_yscale('log')\n# ax.set_title('MSE_Loss Loss')\n# ax.set_xlabel('Iteration')","metadata":{"execution":{"iopub.status.busy":"2022-05-23T12:15:53.381Z","iopub.execute_input":"2022-05-23T12:15:53.381809Z","iopub.status.idle":"2022-05-23T12:15:53.594753Z","shell.execute_reply.started":"2022-05-23T12:15:53.381769Z","shell.execute_reply":"2022-05-23T12:15:53.594027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss = chkpt['loss']['train_loss_ad'][:1000]\n# f = plt.figure(figsize=(20,8))\n# ax = f.add_subplot(1,2,1)\n# ax.plot(loss)\n# # ax.set_yscale('log')\n# ax.set_title('MSE_Loss Loss')\n# ax.set_xlabel('Iteration')","metadata":{"execution":{"iopub.status.busy":"2022-05-23T12:15:35.750563Z","iopub.execute_input":"2022-05-23T12:15:35.751262Z","iopub.status.idle":"2022-05-23T12:15:35.931008Z","shell.execute_reply.started":"2022-05-23T12:15:35.751217Z","shell.execute_reply":"2022-05-23T12:15:35.930256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mask(img):\n        canvas = np.full((224,224,3), 255, np.uint8) #draw white canvas\n        \n        for _ in range(np.random.randint(1,10)):\n            x1, x2 = np.random.randint(1,224), np.random.randint(1,224)\n            y1, y2 = np.random.randint(1,224), np.random.randint(1,224)\n            t = np.random.randint(1,3)\n            \n            cv2.line(canvas,(x1,y1),(x2,y2),(0,0,0),t)\n            \n            \n        masked_img = img.copy()\n        masked_img[canvas==0] = 255\n            \n        return masked_img, canvas\n    \nip_img = Image.open('../input/internet-imgs/Sardar_patel_(cropped).jpg').convert(\"RGB\")\n# ip_img = Image.open('../input/internet-imgs/netaji-subash-chandra-bose1611225337348.jpg').convert(\"RGB\")\nip_img = ip_img.resize((224,224))\nb_w = copy.deepcopy(ip_img)\nip_img = np.array(ip_img)\n\nmasked_L = copy.deepcopy(ip_img)\nmasked_L,_ = mask(masked_L)\n\nmasked_L = rgb2lab(masked_L).astype(\"float32\")\nmasked_L = transforms.ToTensor()(masked_L)\nmasked_L = masked_L[[0]] / 50. - 1.\n# print(ip_img.shape)\n\nimg_lab = rgb2lab(ip_img).astype(\"float32\") # Converting RGB to L*a*b\nimg_lab = transforms.ToTensor()(img_lab)\n\nL = img_lab[[0]] / 50. - 1. # Max for L is 100 so need /50 -1 to get b/w -1,1\nab = img_lab[[1,2]]/100. #Max for a and b is 127\nL=torch.unsqueeze(L,dim=0).cuda()\nmasked_L=torch.unsqueeze(masked_L,dim=0).cuda()\n# print(masked_L.shape)\n# L.shape\n\nchkpt = torch.load(\"../input/adversarial-mixup-105-epoch/_epoch_105\",map_location=torch.device('cpu'))\n# chkpt_inpaint = torch.load(\"../input/inpaint35/inpaint_epoch_35 (1)\",map_location=torch.device('cpu'))\nmodel.load_state_dict(chkpt['model_state_dict'])\n\n# inpaint_model.load_state_dict(chkpt_inpaint['model_state_dict'])\n\nmodel.eval()\nwith torch.no_grad():\n    gray_val = copy.deepcopy(L)\n    true_ab_val_orig = copy.deepcopy(ab)\n    \n#     gray_val = gray_val.to(device)\n\n    pred_ab,_ = model(torch.cat((L,masked_L),dim=0))\n# print(type(pred_ab),pred_ab.size)    \npred_ab = (pred_ab*100.).permute(0, 2, 3, 1).contiguous()\ngray_val = ((gray_val+1)*50.).permute(0,2,3,1).contiguous()\npred_rgb = torch.tensor(lab2rgb(torch.cat((gray_val,pred_ab),dim=3).cpu()))\npred_rgb = pred_rgb.permute(0,3,1,2)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-06T10:41:43.229523Z","iopub.execute_input":"2022-08-06T10:41:43.229956Z","iopub.status.idle":"2022-08-06T10:41:43.727311Z","shell.execute_reply.started":"2022-08-06T10:41:43.229920Z","shell.execute_reply":"2022-08-06T10:41:43.726581Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"plt.imshow(b_w)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T10:41:49.676591Z","iopub.execute_input":"2022-08-06T10:41:49.676934Z","iopub.status.idle":"2022-08-06T10:41:49.893227Z","shell.execute_reply.started":"2022-08-06T10:41:49.676901Z","shell.execute_reply":"2022-08-06T10:41:49.892466Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"show(make_grid(pred_rgb.cpu().data),figure_size=(4,4))","metadata":{"execution":{"iopub.status.busy":"2022-08-06T10:41:50.021264Z","iopub.execute_input":"2022-08-06T10:41:50.021573Z","iopub.status.idle":"2022-08-06T10:41:50.305154Z","shell.execute_reply.started":"2022-08-06T10:41:50.021545Z","shell.execute_reply":"2022-08-06T10:41:50.304467Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Personal Images","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def show_p(img,name,figure_size=(20,20)):\n#     print(img.shape)\n    npimg = img.numpy()\n    npimg = img\n    print(npimg.shape)\n    _, ax = plt.subplots(figsize=figure_size)\n    fig = ax.imshow(np.transpose(npimg, (1,2,0)))\n    fig.axes.get_xaxis().set_visible(True)\n    fig.axes.get_yaxis().set_visible(True)\n    plt.savefig(\"./\"+name)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T11:01:00.409679Z","iopub.execute_input":"2022-08-06T11:01:00.410199Z","iopub.status.idle":"2022-08-06T11:01:00.415841Z","shell.execute_reply.started":"2022-08-06T11:01:00.410162Z","shell.execute_reply":"2022-08-06T11:01:00.414888Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def mask(img):\n        canvas = np.full((224,224,3), 255, np.uint8) #draw white canvas\n        \n        for _ in range(np.random.randint(1,10)):\n            x1, x2 = np.random.randint(1,224), np.random.randint(1,224)\n            y1, y2 = np.random.randint(1,224), np.random.randint(1,224)\n            t = np.random.randint(1,3)\n            \n            cv2.line(canvas,(x1,y1),(x2,y2),(0,0,0),t)\n            \n            \n        masked_img = img.copy()\n        masked_img[canvas==0] = 255\n            \n        return masked_img, canvas\n    \nadd_string = '../input/p-photos/pandit ji.jpeg'\nip_img = Image.open(add_string).convert(\"RGB\")\np_name = add_string.strip('../input/p-photos/').strip('.jpeg')\n\n# ip_img = Image.open('../input/internet-imgs/netaji-subash-chandra-bose1611225337348.jpg').convert(\"RGB\")\nip_img = ip_img.resize((224,224))\nb_w = copy.deepcopy(ip_img)\nip_img = np.array(ip_img)\n\nmasked_L = copy.deepcopy(ip_img)\nmasked_L,_ = mask(masked_L)\n\nmasked_L = rgb2lab(masked_L).astype(\"float32\")\nmasked_L = transforms.ToTensor()(masked_L)\nmasked_L = masked_L[[0]] / 50. - 1.\n# print(ip_img.shape)\n\nimg_lab = rgb2lab(ip_img).astype(\"float32\") # Converting RGB to L*a*b\nimg_lab = transforms.ToTensor()(img_lab)\n\nL = img_lab[[0]] / 50. - 1. # Max for L is 100 so need /50 -1 to get b/w -1,1\nab = img_lab[[1,2]]/100. #Max for a and b is 127\nL=torch.unsqueeze(L,dim=0).cuda()\nmasked_L=torch.unsqueeze(masked_L,dim=0).cuda()\n# print(masked_L.shape)\n# L.shape\n\nchkpt = torch.load(\"../input/adversarial-mixup-105-epoch/_epoch_105\",map_location=torch.device('cpu'))\n# chkpt_inpaint = torch.load(\"../input/inpaint35/inpaint_epoch_35 (1)\",map_location=torch.device('cpu'))\nmodel.load_state_dict(chkpt['model_state_dict'])\n\n# inpaint_model.load_state_dict(chkpt_inpaint['model_state_dict'])\n\nmodel.eval()\nwith torch.no_grad():\n    gray_val = copy.deepcopy(L)\n    true_ab_val_orig = copy.deepcopy(ab)\n    \n#     gray_val = gray_val.to(device)\n\n    pred_ab,_ = model(torch.cat((L,masked_L),dim=0))\n# print(type(pred_ab),pred_ab.size)    \npred_ab = (pred_ab*100.).permute(0, 2, 3, 1).contiguous()\ngray_val = ((gray_val+1)*50.).permute(0,2,3,1).contiguous()\npred_rgb = torch.tensor(lab2rgb(torch.cat((gray_val,pred_ab),dim=3).cpu()))\npred_rgb = pred_rgb.permute(0,3,1,2)\n\nshow_p(make_grid(pred_rgb.cpu().data),p_name,figure_size=(4,4))\n","metadata":{"execution":{"iopub.status.busy":"2022-08-06T11:04:30.551666Z","iopub.execute_input":"2022-08-06T11:04:30.552006Z","iopub.status.idle":"2022-08-06T11:04:31.134945Z","shell.execute_reply.started":"2022-08-06T11:04:30.551975Z","shell.execute_reply":"2022-08-06T11:04:31.133985Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}